# Robot Savo — STT + LLM gateway
# Example environment file for stt_server.
#
# Usage:
#   1) Copy this file to `.env`:
#        cp .env.example .env
#   2) Edit `.env` as needed for your machine (GPU/CPU, URLs, etc.).
#   3) Do NOT commit `.env` to Git. Keep only `.env.example` in the repo.
#
# Notes:
#   - These values are read by app/config.py via Settings.from_env().
#   - docker-compose can either pass them via `environment:` or `env_file:`.


# ---------------------------------------------------------------------------
# STT (faster-whisper) configuration
# ---------------------------------------------------------------------------

# Directory where faster-whisper models are stored inside the container.
# In docker-compose.yml we mount:
#   ./stt_server/models -> /models
STT_MODELS_DIR=/models

# Name or directory of the model under STT_MODELS_DIR.
# If you downloaded "small.en" into stt_server/models/small.en,
# then STT_MODEL_NAME should be "small.en".
STT_MODEL_NAME=small.en

# Device to use for STT inference:
#   - cpu    → always CPU
#   - cuda   → NVIDIA GPU (if available)
#   - auto   → let faster-whisper decide
STT_DEVICE=cpu

# Language hint for faster-whisper.
# Use "en" for English; leave empty to let the model auto-detect.
STT_LANGUAGE=en


# ---------------------------------------------------------------------------
# LLM server configuration (inside Docker network)
# ---------------------------------------------------------------------------

# Base URL for Robot Savo LLM server as seen from inside the stt_server
# container. In docker-compose, the service name "llm_server" is resolvable
# as a hostname, so we use:
#   http://llm_server:8000
LLM_SERVER_URL=http://llm_server:8000

# Timeout (in seconds) for HTTP requests to the LLM /chat endpoint.
LLM_TIMEOUT_S=15.0


# ---------------------------------------------------------------------------
# Identity & logging
# ---------------------------------------------------------------------------

# Robot identifier passed through to the LLM server so logs can distinguish
# which robot instance is speaking.
ROBOT_ID=robot_savo_pi

# Log level for this service:
#   DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
